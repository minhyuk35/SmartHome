"""
[ì¢…í•© PC ë¹„ì„œ]
Whisper ìŒì„± + Google TTS + GUI ì œì–´ + ğŸ”¥ë²„íŠ¼ì‹ ì–¼êµ´ ì¸ì‹ (íŒ¨í‚· ë¶„ë¦¬ + ì¹´ë©”ë¼ ì•ˆì •í™”)
"""

import socket
import time
import whisper
import sounddevice as sd
import numpy as np
from pathlib import Path
import threading
import requests       
import urllib.parse   
import os
from playsound import playsound
import cv2                 
import face_recognition    

# ================================
# âš™ï¸ ì„¤ì •
# ================================
JAVA_IP = "127.0.0.1"    
CMD_PORT = 39186         
VOICE_SERVER_PORT = 40191
DOOR_EVENT_PORT = 39189

# ìƒíƒœ í”Œë˜ê·¸
is_registering_mode = False   
is_active_recognition = False 
my_command_lock = False       

# ================================
# ğŸ”Š TTS ë° í†µì‹ 
# ================================
def speak_answer(text):
    try:
        enc_text = urllib.parse.quote(text)
        url = f"https://translate.google.com/translate_tts?ie=UTF-8&q={enc_text}&tl=ko&client=tw-ob"
        headers = {"User-Agent": "Mozilla/5.0"}
        
        response = requests.get(url, headers=headers)
        filename = "pc_voice_temp.mp3"
        
        if os.path.exists(filename):
            try: os.remove(filename)
            except: pass
            
        with open(filename, 'wb') as f:
            f.write(response.content)
            
        playsound(filename)
        try: os.remove(filename)
        except: pass
    except: pass

def send_to_java(cmd):
    global my_command_lock
    my_command_lock = True
    def release_lock():
        global my_command_lock
        time.sleep(1.5)
        my_command_lock = False
    threading.Thread(target=release_lock).start()

    max_retries = 3
    for attempt in range(max_retries):
        try:
            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            s.settimeout(2)
            s.connect((JAVA_IP, CMD_PORT))
            s.sendall((cmd + "\n").encode())
            s.close()
            print(f"[SEND] ğŸ“¤ JAVA ì „ì†¡: {cmd}")
            return True
        except:
            if attempt < max_retries - 1: time.sleep(0.5)
    return False

# ================================
# ğŸ“¸ ì–¼êµ´ ë“±ë¡ ëª¨ë“œ (ë©”ëª¨ë¦¬ íŒ¨ì¹˜ + ìœˆë„ìš° ì¹´ë©”ë¼ í˜¸í™˜)
# ================================
def start_face_registration():
    global is_registering_mode
    is_registering_mode = True 
    
    print("ğŸ“¸ [ì–¼êµ´ ë“±ë¡] ì¹´ë©”ë¼ ê°€ë™...")
    speak_answer("ì–¼êµ´ ë“±ë¡ ëª¨ë“œì…ë‹ˆë‹¤.")
    
    # ğŸ”¥ [ì¤‘ìš”] ìœˆë„ìš°ì—ì„œëŠ” CAP_DSHOWë¥¼ ì¨ì•¼ ì¹´ë©”ë¼ê°€ ë¹¨ë¦¬ ì¼œì§
    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)
    
    if not cap.isOpened():
        print("âŒ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        speak_answer("ì¹´ë©”ë¼ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        is_registering_mode = False
        return

    while True:
        ret, frame = cap.read()
        if not ret: 
            print("âŒ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            break
        
        cv2.putText(frame, "Press 's' to Save, 'q' to Quit", (50, 50), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        cv2.imshow('Register Face', frame)
        
        key = cv2.waitKey(1) & 0xFF
        if key == ord('s'): 
            try:
                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                rgb = np.ascontiguousarray(rgb, dtype=np.uint8)
                boxes = face_recognition.face_locations(rgb)
                
                if boxes:
                    enc = face_recognition.face_encodings(rgb, boxes)[0]
                    np.save("owner_face.npy", enc)
                    print("âœ… ì–¼êµ´ ì €ì¥ ì™„ë£Œ")
                    speak_answer("ì–¼êµ´ì´ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    break
                else:
                    print("âŒ ì–¼êµ´ ë¯¸ê°ì§€")
                    speak_answer("ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                print(f"âŒ ë“±ë¡ ì—ëŸ¬: {e}")
                speak_answer("ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

        elif key == ord('q'):
            print("ì·¨ì†Œë¨")
            speak_answer("ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
            break
            
    cap.release()
    cv2.destroyAllWindows()
    is_registering_mode = False
    print("ğŸ‘€ ë‹¤ì‹œ ëŒ€ê¸° ëª¨ë“œ")

# ================================
# ğŸ‘ï¸ [í•µì‹¬] ë²„íŠ¼ì‹ ì–¼êµ´ ì¸ì‹ ìŠ¤ë ˆë“œ (ì¿¨íƒ€ì„+ì¹´ë©”ë¼ ì•ˆì •í™”)
# ================================
def face_recognition_loop():
    global is_active_recognition, is_registering_mode
    print("[Face] ğŸ™‚ ëŒ€ê¸° ì¤‘ (ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ì¼œì§‘ë‹ˆë‹¤)")
    
    video_capture = None
    last_unlock_time = 0 

    while True:
        current_time = time.time()
        is_cooldown = (current_time - last_unlock_time < 10)

        # 1. ì¹´ë©”ë¼ ë„ê¸° ì¡°ê±´ (ë¹„í™œì„± OR ë“±ë¡ì¤‘ OR ì¿¨íƒ€ì„)
        if not is_active_recognition or is_registering_mode or is_cooldown:
            if video_capture is not None:
                video_capture.release()
                video_capture = None
                if is_cooldown: print(f"[Face] â³ ì¿¨íƒ€ì„... {int(10 - (current_time - last_unlock_time))}ì´ˆ ë‚¨ìŒ")
                else: print("[Face] ğŸ’¤ ì¹´ë©”ë¼ êº¼ì§ (ëŒ€ê¸°)")
            
            time.sleep(1)
            continue

        # 2. ì¹´ë©”ë¼ ì¼œê¸°
        if video_capture is None:
            # ğŸ”¥ [ì¤‘ìš”] CAP_DSHOW ì¶”ê°€ (ìœˆë„ìš° ì „ìš©)
            video_capture = cv2.VideoCapture(0, cv2.CAP_DSHOW)
            
            if not video_capture.isOpened():
                speak_answer("ì¹´ë©”ë¼ë¥¼ ì¼¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                is_active_recognition = False
                continue
            print("[Face] ğŸ“¸ ì¹´ë©”ë¼ ON! ì–¼êµ´ ì°¾ëŠ” ì¤‘...")

        # 3. ë°ì´í„° ë¡œë“œ (íŒŒì¼ ì—†ì„ ë•Œ ì˜ˆì™¸ì²˜ë¦¬)
        try: owner_encoding = np.load("owner_face.npy")
        except: 
            speak_answer("ë¨¼ì € ì–¼êµ´ ë“±ë¡ì„ í•´ì£¼ì„¸ìš”.")
            is_active_recognition = False
            continue

        ret, frame = video_capture.read()
        if not ret: continue

        # 4. ì¸ì‹ ì‹œë„
        try:
            small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)
            rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)
            rgb_small_frame = np.ascontiguousarray(rgb_small_frame, dtype=np.uint8)
            
            face_locations = face_recognition.face_locations(rgb_small_frame)
            
            if face_locations:
                face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)
                for face_encoding in face_encodings:
                    matches = face_recognition.compare_faces([owner_encoding], face_encoding, tolerance=0.45)
                    
                    if True in matches:
                        print("[Face] ğŸ”“ ì£¼ì¸ë‹˜ í™•ì¸ë¨!")
                        speak_answer("ì£¼ì¸ë‹˜ì´ì‹œêµ°ìš”. ë¬¸ì„ ì—´ì–´ë“œë¦½ë‹ˆë‹¤.")
                        send_to_java("UNLOCK")
                        
                        last_unlock_time = time.time() # ì¿¨íƒ€ì„ ì‹œì‘
                        is_active_recognition = False  # ì¹´ë©”ë¼ ë„ê¸° ìš”ì²­
                        break 
        except: pass

    if video_capture is not None:
        video_capture.release()

face_thread = threading.Thread(target=face_recognition_loop, daemon=True)
face_thread.start()


# ================================
# ğŸ‘‚ [ìˆ˜ì •ë¨] Java GUI ë²„íŠ¼ ê°ì‹œì (íŒ¨í‚· ë¶„ë¦¬ ì ìš©)
# ================================
def listen_java_commands():
    global is_active_recognition
    print("[Thread] ğŸ‘ï¸ GUI ë²„íŠ¼ ê°ì‹œ ì‹œì‘...")
    
    while True:
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.connect((JAVA_IP, CMD_PORT))
            while True:
                # ë°ì´í„° ìˆ˜ì‹ 
                raw_data = sock.recv(1024).decode()
                if not raw_data: break
                
                # ğŸ”¥ [í•µì‹¬ ìˆ˜ì •] ë­‰ì³ì˜¨ ë°ì´í„° ìª¼ê°œê¸° (split)
                commands = raw_data.split('\n')
                
                for data in commands:
                    data = data.strip()
                    if not data: continue # ë¹ˆ ì¤„ ë¬´ì‹œ
                    
                    if my_command_lock: continue

                    print(f"[GUI ìˆ˜ì‹ ] {data}")
                    
                    if data == "REQ_FACE_UNLOCK":
                        print("ğŸ“¸ ì–¼êµ´ ì¸ì‹ ìš”ì²­ë¨! (10ì´ˆ íƒ€ì„ì•„ì›ƒ)")
                        speak_answer("ì •ë©´ì„ ë´ì£¼ì„¸ìš”.")
                        is_active_recognition = True
                        
                        # íƒ€ì„ì•„ì›ƒ ìŠ¤ë ˆë“œ
                        def timeout_timer():
                            time.sleep(10)
                            global is_active_recognition
                            if is_active_recognition:
                                print("â° íƒ€ì„ì•„ì›ƒ")
                                is_active_recognition = False
                                speak_answer("ì–¼êµ´ì´ í™•ì¸ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                        threading.Thread(target=timeout_timer).start()

                    elif data == "REGISTER_FACE":
                        threading.Thread(target=start_face_registration).start()
                    
                    elif data == "LED_ON":       speak_answer("ì¡°ëª…ì„ ì¼°ìŠµë‹ˆë‹¤.")
                    elif data == "LED_OFF":      speak_answer("ì¡°ëª…ì„ ê»ìŠµë‹ˆë‹¤.")
                    elif data == "FAN_ON":       speak_answer("ì„ í’ê¸°ë¥¼ ì¼°ìŠµë‹ˆë‹¤.")
                    elif data == "FAN_OFF":      speak_answer("ì„ í’ê¸°ë¥¼ ê»ìŠµë‹ˆë‹¤.")
                    elif data == "LIGHT_SLEEP":  speak_answer("ìˆ˜ë©´ ëª¨ë“œë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")
                    elif data == "LIGHT_WARM":   speak_answer("ë”°ëœ»í•œ ì¡°ëª…ìœ¼ë¡œ ë°”ê¿¨ìŠµë‹ˆë‹¤.")
                    elif data == "RGB_ON":       speak_answer("ë¬´ë“œë“±ì„ ì¼°ìŠµë‹ˆë‹¤.")
                    elif data == "RGB_OFF":      speak_answer("ë¬´ë“œë“±ì„ ê»ìŠµë‹ˆë‹¤.")
                    elif data == "UNLOCK":       speak_answer("ë¬¸ì„ ì—´ì—ˆìŠµë‹ˆë‹¤.")

            sock.close()
        except: time.sleep(3)

cmd_thread = threading.Thread(target=listen_java_commands, daemon=True)
cmd_thread.start()

# ================================
# ğŸ”¥ Whisper STT & Logic (ê¸°ì¡´ ìœ ì§€)
# ================================
print("ğŸ“¢ Whisper ëª¨ë¸ ë¡œë”© ì¤‘...")
model = whisper.load_model("base", device="cpu")
print("âœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ")

running_event = threading.Event()
SAMPLE_RATE = 16000
is_recording = False
audio_chunks = []
stream = None

def start_recording():
    global is_recording, stream, audio_chunks
    is_recording = True
    audio_chunks = []
    print("ğŸ¤ ë…¹ìŒ ì‹œì‘...")
    stream = sd.InputStream(channels=1, samplerate=SAMPLE_RATE, dtype=np.float32)
    if stream is not None: stream.start()
    
    def recording_thread():
        global stream
        while is_recording:
            try:
                if stream is not None:
                    chunk, _ = stream.read(SAMPLE_RATE // 10)
                    if chunk is not None: audio_chunks.append(chunk)
            except: break
        if stream is not None: stream.stop(); stream.close()
    threading.Thread(target=recording_thread, daemon=True).start()

def stop_recording():
    global is_recording, audio_chunks
    is_recording = False
    time.sleep(0.5)
    if not audio_chunks: return None
    return np.concatenate(audio_chunks, axis=0)

def audio_to_file(audio):
    temp = Path(__file__).parent / "temp_audio.wav"
    import soundfile as sf
    sf.write(str(temp), audio, SAMPLE_RATE)
    return str(temp)

def process_command(text):
    print(f"[STT] ğŸ—£ï¸ {text}")
    text = text.lower()
    if "ë¶ˆ ì¼œ" in text:
        speak_answer("ë„¤, ì¡°ëª…ì„ ì¼œê² ìŠµë‹ˆë‹¤.")
        send_to_java("LED_ON")
    elif "ë¶ˆ êº¼" in text:
        speak_answer("ì¡°ëª…ì„ ë•ë‹ˆë‹¤.")
        send_to_java("LED_OFF")
    elif "ì„ í’ê¸° ì¼œ" in text:
        speak_answer("ì„ í’ê¸°ë¥¼ ì¼­ë‹ˆë‹¤.")
        send_to_java("FAN_ON")
    elif "ì„ í’ê¸° êº¼" in text:
        speak_answer("ì„ í’ê¸°ë¥¼ ë•ë‹ˆë‹¤.")
        send_to_java("FAN_OFF")
    elif "ë¬¸ ì—´ì–´" in text:
        speak_answer("ë¬¸ì„ ì—½ë‹ˆë‹¤.")
        send_to_java("UNLOCK")

def stop_recording_and_process():
    audio = stop_recording()
    if audio is None: return
    f = audio_to_file(audio)
    try:
        res = model.transcribe(f, language="ko", verbose=False)
        txt = str(res["text"]).strip()
        if txt: process_command(txt)
    except: pass
    try: os.remove(f)
    except: pass

def listen_door_events():
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.connect((JAVA_IP, DOOR_EVENT_PORT))
        while True:
            data = sock.recv(1024).decode().strip()
            if not data: break
            if data == "UNLOCKED":
                start_recording()
        sock.close()
    except: pass
threading.Thread(target=listen_door_events, daemon=True).start()

def listen_voice_server():
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    try: s.bind(("127.0.0.1", VOICE_SERVER_PORT)); s.listen(5)
    except: return
    while True:
        try:
            c, _ = s.accept()
            d = c.recv(1024).decode().strip()
            if d == "START_RECORDING": start_recording()
            elif d == "STOP_RECORDING": stop_recording_and_process()
            c.close()
        except: pass
threading.Thread(target=listen_voice_server, daemon=True).start()

print("\n=== [PC ë¹„ì„œ ì‹œìŠ¤í…œ ê°€ë™] ===")
print("1. ìŒì„± ì¸ì‹ (Whisper)")
print("2. GUI ì—°ë™ (Toss Style)")
print("3. ì–¼êµ´ ì¸ì‹/ë“±ë¡ (ì ˆì „ ëª¨ë“œ)")
print("============================")

try:
    while True: time.sleep(1)
except KeyboardInterrupt: print("ì¢…ë£Œ")